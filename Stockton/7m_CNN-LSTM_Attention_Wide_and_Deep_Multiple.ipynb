{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras_tuner import RandomSearch\n",
    "from GEH_LOSS import GEH_loss\n",
    "import geh as ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import relative_accuracy as ra\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all Inputs and Output Data\n",
    "\n",
    "# 5 mins (1 step ahead)\n",
    "Deep_train_5   = np.load(\"Deep_train_5_multi.npz\")['x'] \n",
    "Output_train_5 = np.load(\"Deep_train_5_multi.npz\")['y']\n",
    "\n",
    "Deep_test_5   = np.load(\"Deep_test_5_multi.npz\")['x'] \n",
    "Output_test_5 = np.load(\"Deep_test_5_multi.npz\")['y'] \n",
    "\n",
    "# 15 mins (3 steps ahead)\n",
    "Deep_train_15   = np.load(\"Deep_train_15_multi.npz\")['x'] \n",
    "Output_train_15 = np.load(\"Deep_train_15_multi.npz\")['y']\n",
    "\n",
    "Deep_test_15   = np.load(\"Deep_test_15_multi.npz\")['x'] \n",
    "Output_test_15 = np.load(\"Deep_test_15_multi.npz\")['y']\n",
    "\n",
    "# 30 mins (6 steps ahead)\n",
    "Deep_train_30   = np.load(\"Deep_train_30_multi.npz\")['x'] \n",
    "Output_train_30 = np.load(\"Deep_train_30_multi.npz\")['y']\n",
    "\n",
    "Deep_test_30   = np.load(\"Deep_test_30_multi.npz\")['x'] \n",
    "Output_test_30 = np.load(\"Deep_test_30_multi.npz\")['y']\n",
    "\n",
    "# 60 mins (12 steps ahead)\n",
    "Deep_train_60   = np.load(\"Deep_train_60_multi.npz\")['x'] \n",
    "Output_train_60 = np.load(\"Deep_train_60_multi.npz\")['y']\n",
    "\n",
    "Deep_test_60   = np.load(\"Deep_test_60_multi.npz\")['x'] \n",
    "Output_test_60 = np.load(\"Deep_test_60_multi.npz\")['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide Data\n",
    "Xtrain = joblib.load(\"002weeks_train.save\") \n",
    "Xtest = joblib.load(\"002weeks_test.save\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 mins\n",
    "# Delete first 15 samples\n",
    "Wide_train_5 = np.delete(Xtrain, np.s_[0:15], 0)\n",
    "Wide_test_5 = np.delete(Xtest, np.s_[0:15], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 mins\n",
    "# Delete first 17 samples\n",
    "Wide_train_15 = np.delete(Xtrain, np.s_[0:17], 0)\n",
    "Wide_test_15 = np.delete(Xtest, np.s_[0:17], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 mins\n",
    "# Delete first 20 samples\n",
    "Wide_train_30 = np.delete(Xtrain, np.s_[0:20], 0)\n",
    "Wide_test_30 = np.delete(Xtest, np.s_[0:20], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60 mins\n",
    "# Delete first 27 samples\n",
    "Wide_train_60 = np.delete(Xtrain, np.s_[0:26], 0)\n",
    "Wide_test_60 = np.delete(Xtest, np.s_[0:26], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"scaler.save\"\n",
    "scaler = joblib.load(scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test      = pd.read_csv('01test_scaled.csv', index_col=None, parse_dates=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to split the input sequences into subsequences that can be processed by the CNN model. Here, each spatio-temporal sample can be split into three sub-samples, each with five time steps. The CNN can interpret each subsequence of five time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 7                # No of loop detectors\n",
    "n_seq = 3                     # Subsequences\n",
    "n_steps = 5                   # time-step per subsequence\n",
    "# val_percent = 0.12962       # 2 weeks\n",
    "val_percent = 0.2         \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xt5 = Deep_train_5.reshape((Deep_train_5.shape[0], n_seq, n_steps, n_features))\n",
    "yt5 = scaler.inverse_transform(Output_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xv5 = Deep_test_5.reshape((Deep_test_5.shape[0], n_seq, n_steps, n_features))\n",
    "yv5 = scaler.inverse_transform(Output_test_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xt15 = Deep_train_15.reshape((Deep_train_15.shape[0], n_seq, n_steps, n_features))\n",
    "yt15 = scaler.inverse_transform(Output_train_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xv15 = Deep_test_15.reshape((Deep_test_15.shape[0], n_seq, n_steps, n_features))\n",
    "yv15 = scaler.inverse_transform(Output_test_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xt30 = Deep_train_30.reshape((Deep_train_30.shape[0], n_seq, n_steps, n_features))\n",
    "yt30 = scaler.inverse_transform(Output_train_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xv30 = Deep_test_30.reshape((Deep_test_30.shape[0], n_seq, n_steps, n_features))\n",
    "yv30 = scaler.inverse_transform(Output_test_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 60 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xt60 = Deep_train_60.reshape((Deep_train_60.shape[0], n_seq, n_steps, n_features))\n",
    "yt60 = scaler.inverse_transform(Output_train_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xv60 = Deep_test_60.reshape((Deep_test_60.shape[0], n_seq, n_steps, n_features))\n",
    "yv60 = scaler.inverse_transform(Output_test_60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Data for Conv2D layer\n",
    "\n",
    "For Conv2D, there is a need to add one more dimension to show we're dealing with 1 channel (since technically the images are in black and white, only showing values from 0-max flow on a single channel).\n",
    "\n",
    "Conv1D - strides in 1 dimension\n",
    "Conv2D - strides in 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define no_of_channels\n",
    "n_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "Xt5  =  Xt5.reshape(Xt5.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xt15 =  Xt15.reshape(Xt15.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xt30 =  Xt30.reshape(Xt30.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xt60 =  Xt60.reshape(Xt60.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "\n",
    "# Validation data\n",
    "Xv5  =  Xv5.reshape(Xv5.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xv15 =  Xv15.reshape(Xv15.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xv30 =  Xv30.reshape(Xv30.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xv60 =  Xv60.reshape(Xv60.shape[0], n_seq, n_steps, n_features, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wt5  = Wide_train_5\n",
    "Wt15 = Wide_train_15\n",
    "Wt30 = Wide_train_30\n",
    "Wt60 = Wide_train_60\n",
    "\n",
    "Wv5  = Wide_test_5\n",
    "Wv15 = Wide_test_15\n",
    "Wv30 = Wide_test_30\n",
    "Wv60 = Wide_test_60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "# Early Stopping\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wide_train_5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 mins ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_5(hp):\n",
    "    # Inputs\n",
    "    channel_Wide = keras.layers.Input(shape=Wide_train_5.shape[1:], name=\"WideInput\")\n",
    "    channel_Deep = keras.layers.Input(shape=Xt5.shape[1:], name=\"DeepInput\")\n",
    "    units = hp.Int(\"units\", min_value=1, max_value=100, step=1)\n",
    "    \n",
    "    \n",
    "    # Wide Model\n",
    "    Wide_5 = keras.layers.Dense(units=units, activation=\"relu\")(channel_Wide)\n",
    "    Wide_5_exp = keras.layers.Reshape(target_shape=(1,-1))(Wide_5)\n",
    "    \n",
    "    \n",
    "    # Deep Model\n",
    "    # CNN \n",
    "    CNN_5 = keras.layers.TimeDistributed(\n",
    "        keras.layers.Conv2D(filters=hp.Int(\"filters\", min_value=32, max_value=512, step=32),\n",
    "                            kernel_size=hp.Choice(\"kernel_size\", [2, 3]), activation='relu'))(channel_Deep)\n",
    "    flatten_5 = keras.layers.TimeDistributed(keras.layers.Flatten())(CNN_5)\n",
    "    # LSTM          \n",
    "    LSTM_5 = keras.layers.LSTM(units=units, activation='relu', return_sequences=True)(flatten_5)\n",
    "\n",
    "    \n",
    "    # Concatenation \n",
    "    concat = keras.layers.concatenate([Wide_5_exp, LSTM_5], axis=1)\n",
    "\n",
    "    # Attention\n",
    "    Att_5 = SeqSelfAttention(attention_activation='sigmoid')(concat)\n",
    "    merge = keras.layers.Flatten()(Att_5)\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    output = keras.layers.Dense(n_features, name= \"output\")(merge)\n",
    "    \n",
    "    # Model\n",
    "    model_5 = keras.Model(inputs=[channel_Wide,channel_Deep], outputs=[output])\n",
    "\n",
    "    # Compile\n",
    "    model_5.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])), \n",
    "        loss=GEH_loss, metrics=['MeanAbsoluteError','RootMeanSquaredError','MeanAbsolutePercentageError'])\n",
    "                    \n",
    "    return model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner5 = RandomSearch(\n",
    "    build_model_5,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=30,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=os.path.normpath('C:/Runs'),\n",
    "    project_name=\"7m-CNN-LSTM-5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner5.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner5.search((Wt5,Xt5), yt5, epochs=200,\n",
    "           validation_split = val_percent,\n",
    "           callbacks = [early_stop],\n",
    "           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner5.results_summary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp5 = tuner5.get_best_hyperparameters()[0]\n",
    "model5 = tuner5.hypermodel.build(best_hp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/7m-ConvLSTM-5\"\n",
    "joblib.dump(best_hp5, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.fit((Wt5,Xt5), yt5, \n",
    "           epochs = 100, \n",
    "           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.save(\"my_dir/7m-CNN-LSTM-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics5 = pd.DataFrame(model5.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics5[['loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = keras.models.load_model(\"my_dir/7m-CNN-LSTM-5\", custom_objects={'GEH_loss': GEH_loss}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.predict((Wv5,Xv5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions5 = model5.predict((Wv5,Xv5))\n",
    "\n",
    "# yv5 = scaler.inverse_transform(Test.drop('Timestamp', axis = 1))\n",
    "# yv5 = yv5[15:,3]\n",
    "\n",
    "# df = Test.drop(Test.head(15).index)\n",
    "# df.reset_index(inplace = True)\n",
    "# df = df.drop(columns=['index','Timestamp'])\n",
    "\n",
    "\n",
    "# df['f4'] = pd.Series(test_predictions5.flatten())\n",
    "\n",
    "# test_predictions5 = scaler.inverse_transform(df)\n",
    "\n",
    "# test_predictions5 = test_predictions5[:,3]\n",
    "\n",
    "# MSE5 = mean_squared_error(yv5,test_predictions5)\n",
    "\n",
    "# MAE5 = mean_absolute_error(yv5,test_predictions5)\n",
    "\n",
    "# RMSE5 = np.sqrt(MSE5)\n",
    "\n",
    "# MAPE5 = mean_absolute_percentage_error(yv5,test_predictions5)*100\n",
    "\n",
    "# eval5 = [MSE5, MAE5, RMSE5, MAPE5]\n",
    "# print(eval5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RA10 = ra.RA_10(np.asarray(yv5), np.asarray(test_predictions5))\n",
    "# RA15 = ra.RA_15(np.asarray(yv5), np.asarray(test_predictions5))\n",
    "# RA20 = ra.RA_20(np.asarray(yv5), np.asarray(test_predictions5))\n",
    "\n",
    "# res5 = [RA10, RA15, RA20]\n",
    "# print(res5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 mins ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_15(hp):\n",
    "    # Inputs\n",
    "    channel_Wide = keras.layers.Input(shape=Wide_train_15.shape[1:], name=\"WideInput\")\n",
    "    channel_Deep = keras.layers.Input(shape=Xt15.shape[1:], name=\"DeepInput\")\n",
    "    units = hp.Int(\"units\", min_value=1, max_value=100, step=1)\n",
    "    \n",
    "    \n",
    "    # Wide Model\n",
    "    Wide_15 = keras.layers.Dense(units=units, activation=\"relu\")(channel_Wide)\n",
    "    Wide_15_exp = keras.layers.Reshape(target_shape=(1,-1))(Wide_15)\n",
    "    \n",
    "    \n",
    "    # Deep Model\n",
    "    # CNN \n",
    "    CNN_15 = keras.layers.TimeDistributed(\n",
    "        keras.layers.Conv2D(filters=hp.Int(\"filters\", min_value=32, max_value=512, step=32),\n",
    "                            kernel_size=hp.Choice(\"kernel_size\", [2, 3]), activation='relu'))(channel_Deep)\n",
    "    flatten_15 = keras.layers.TimeDistributed(keras.layers.Flatten())(CNN_15)\n",
    "    # LSTM          \n",
    "    LSTM_15 = keras.layers.LSTM(units=units, activation='relu', return_sequences=True)(flatten_15)\n",
    "\n",
    "    \n",
    "    # Concatenation \n",
    "    concat = keras.layers.concatenate([Wide_15_exp, LSTM_15], axis=1)\n",
    "\n",
    "    # Attention\n",
    "    Att_15 = SeqSelfAttention(attention_activation='sigmoid')(concat)\n",
    "    merge = keras.layers.Flatten()(Att_15)\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    output = keras.layers.Dense(n_features, name= \"output\")(merge)\n",
    "    \n",
    "    # Model\n",
    "    model_15 = keras.Model(inputs=[channel_Wide,channel_Deep], outputs=[output])\n",
    "\n",
    "    # Compile\n",
    "    model_15.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])), \n",
    "        loss=GEH_loss, metrics=['MeanAbsoluteError','RootMeanSquaredError','MeanAbsolutePercentageError'])\n",
    "    \n",
    "    return model_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner15 = RandomSearch(\n",
    "    build_model_15,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=30,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=os.path.normpath('C:/Runs'),\n",
    "    project_name=\"7m-CNN-LSTM-15\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner15.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner15.search((Wt15,Xt15), yt15, epochs=200,\n",
    "            validation_split = val_percent,\n",
    "            callbacks=[early_stop], \n",
    "            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner15.results_summary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp15 = tuner15.get_best_hyperparameters()[0]\n",
    "model15 = tuner15.hypermodel.build(best_hp15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/7m-ConvLSTM-15\"\n",
    "joblib.dump(best_hp15, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15.fit((Wt15,Xt15), yt15, \n",
    "            epochs = 100, \n",
    "            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15.save(\"my_dir/7m-CNN-LSTM-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics15 = pd.DataFrame(model15.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics15[['loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15 = keras.models.load_model(\"my_dir/7m-CNN-LSTM-15\", custom_objects={'GEH_loss': GEH_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15.predict((Wv15,Xv15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions15 = model15.predict((Wv15,Xv15))\n",
    "\n",
    "# yv15 = scaler.inverse_transform(Test.drop('Timestamp', axis = 1))\n",
    "# yv15 = yv15[17:,3]\n",
    "\n",
    "# df = Test.drop(Test.head(17).index)\n",
    "# df.reset_index(inplace = True)\n",
    "# df = df.drop(columns=['index','Timestamp'])\n",
    "# df['f4'] = pd.Series(test_predictions15.flatten())\n",
    "\n",
    "# test_predictions15 = scaler.inverse_transform(df)\n",
    "\n",
    "# test_predictions15 = test_predictions15[:,3]\n",
    "\n",
    "# MSE15 = mean_squared_error(yv15,test_predictions15)\n",
    "\n",
    "# MAE15 = mean_absolute_error(yv15,test_predictions15)\n",
    "\n",
    "# RMSE15 = np.sqrt(MSE15)\n",
    "\n",
    "# MAPE15 = mean_absolute_percentage_error(yv15,test_predictions15)*100\n",
    "\n",
    "# eval15 = [MSE15, MAE15, RMSE15, MAPE15]\n",
    "# print(eval15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RA10 = ra.RA_10(np.asarray(yv15), np.asarray(test_predictions15))\n",
    "# RA15 = ra.RA_15(np.asarray(yv15), np.asarray(test_predictions15))\n",
    "# RA20 = ra.RA_20(np.asarray(yv15), np.asarray(test_predictions15))\n",
    "\n",
    "# res15 = [RA10, RA15, RA20]\n",
    "# print(res15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 mins ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_30(hp):\n",
    "    # Inputs\n",
    "    channel_Wide = keras.layers.Input(shape=Wide_train_30.shape[1:], name=\"WideInput\")\n",
    "    channel_Deep = keras.layers.Input(shape=Xt30.shape[1:], name=\"DeepInput\")\n",
    "    units = hp.Int(\"units\", min_value=1, max_value=100, step=1)\n",
    "    \n",
    "    \n",
    "    # Wide Model\n",
    "    Wide_30 = keras.layers.Dense(units=units, activation=\"relu\")(channel_Wide)\n",
    "    Wide_30_exp = keras.layers.Reshape(target_shape=(1,-1))(Wide_30)\n",
    "    \n",
    "    \n",
    "    # Deep Model\n",
    "    # CNN \n",
    "    CNN_30 = keras.layers.TimeDistributed(\n",
    "        keras.layers.Conv2D(filters=hp.Int(\"filters\", min_value=32, max_value=512, step=32),\n",
    "                            kernel_size=hp.Choice(\"kernel_size\", [2, 3]), activation='relu'))(channel_Deep)\n",
    "    flatten_30 = keras.layers.TimeDistributed(keras.layers.Flatten())(CNN_30)\n",
    "    # LSTM          \n",
    "    LSTM_30 = keras.layers.LSTM(units=units, activation='relu', return_sequences=True)(flatten_30)\n",
    "\n",
    "    \n",
    "    # Concatenation \n",
    "    concat = keras.layers.concatenate([Wide_30_exp, LSTM_30], axis=1)\n",
    "\n",
    "    # Attention\n",
    "    Att_30 = SeqSelfAttention(attention_activation='sigmoid')(concat)\n",
    "    merge = keras.layers.Flatten()(Att_30)\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    output = keras.layers.Dense(n_features, name= \"output\")(merge)\n",
    "    \n",
    "    # Model\n",
    "    model_30 = keras.Model(inputs=[channel_Wide,channel_Deep], outputs=[output])\n",
    "\n",
    "    # Compile\n",
    "    model_30.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])), \n",
    "        loss=GEH_loss, metrics=['MeanAbsoluteError','RootMeanSquaredError','MeanAbsolutePercentageError'])\n",
    "    \n",
    "    return model_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner30 = RandomSearch(\n",
    "    build_model_30,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=30,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=os.path.normpath('C:/Runs'),\n",
    "    project_name=\"7m-CNN-LSTM-30\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner30.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner30.search((Wt30,Xt30), yt30, epochs=200,\n",
    "            validation_split = val_percent,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner30.results_summary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp30 = tuner30.get_best_hyperparameters()[0]\n",
    "model30 = tuner30.hypermodel.build(best_hp30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"7m-ConvLSTM-30\"\n",
    "joblib.dump(best_hp30, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30.fit((Wt30,Xt30), yt30, \n",
    "            epochs = 100, \n",
    "            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30.save(\"my_dir/7m-CNN-LSTM-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics30 = pd.DataFrame(model30.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics30[['loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30 = keras.models.load_model(\"my_dir/7m-CNN-LSTM-30\", custom_objects={'GEH_loss': GEH_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30.predict((Wv30,Xv30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions30 = model30.predict((Wv30,Xv30))\n",
    "\n",
    "# yv30 = scaler.inverse_transform(Test.drop('Timestamp', axis = 1))\n",
    "# yv30 = yv30[20:,3]\n",
    "\n",
    "# df = Test.drop(Test.head(20).index)\n",
    "# df.reset_index(inplace = True)\n",
    "# df = df.drop(columns=['index','Timestamp'])\n",
    "# df['f4'] = pd.Series(test_predictions30.flatten())\n",
    "\n",
    "# test_predictions30 = scaler.inverse_transform(df)\n",
    "\n",
    "# test_predictions30 = test_predictions30[:,3]\n",
    "\n",
    "# MSE30 = mean_squared_error(yv30,test_predictions30)\n",
    "\n",
    "# MAE30 = mean_absolute_error(yv30,test_predictions30)\n",
    "\n",
    "# RMSE30 = np.sqrt(MSE30)\n",
    "\n",
    "# MAPE30 = mean_absolute_percentage_error(yv30,test_predictions30)*100\n",
    "\n",
    "# eval30 = [MSE30, MAE30, RMSE30, MAPE30]\n",
    "# print(eval30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RA10 = ra.RA_10(np.asarray(yv30), np.asarray(test_predictions30))\n",
    "# RA15 = ra.RA_15(np.asarray(yv30), np.asarray(test_predictions30))\n",
    "# RA20 = ra.RA_20(np.asarray(yv30), np.asarray(test_predictions30))\n",
    "\n",
    "# res30 = [RA10, RA15, RA20]\n",
    "# print(res30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 60 mins ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_60(hp):\n",
    "    # Inputs\n",
    "    channel_Wide = keras.layers.Input(shape=Wide_train_60.shape[1:], name=\"WideInput\")\n",
    "    channel_Deep = keras.layers.Input(shape=Xt60.shape[1:], name=\"DeepInput\")\n",
    "    units = hp.Int(\"units\", min_value=1, max_value=100, step=1)\n",
    "    \n",
    "    \n",
    "    # Wide Model\n",
    "    Wide_60 = keras.layers.Dense(units=units, activation=\"relu\")(channel_Wide)\n",
    "    Wide_60_exp = keras.layers.Reshape(target_shape=(1,-1))(Wide_60)\n",
    "    \n",
    "    \n",
    "    # Deep Model\n",
    "    # CNN \n",
    "    CNN_60 = keras.layers.TimeDistributed(\n",
    "        keras.layers.Conv2D(filters=hp.Int(\"filters\", min_value=32, max_value=512, step=32),\n",
    "                            kernel_size=hp.Choice(\"kernel_size\", [2, 3]), activation='relu'))(channel_Deep)\n",
    "    flatten_60 = keras.layers.TimeDistributed(keras.layers.Flatten())(CNN_60)\n",
    "    # LSTM          \n",
    "    LSTM_60 = keras.layers.LSTM(units=units, activation='relu', return_sequences=True)(flatten_60)\n",
    "\n",
    "    \n",
    "    # Concatenation \n",
    "    concat = keras.layers.concatenate([Wide_60_exp, LSTM_60], axis=1)\n",
    "\n",
    "    # Attention\n",
    "    Att_60 = SeqSelfAttention(attention_activation='sigmoid')(concat)\n",
    "    merge = keras.layers.Flatten()(Att_60)\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    output = keras.layers.Dense(n_features, name= \"output\")(merge)\n",
    "    \n",
    "    # Model\n",
    "    model_60 = keras.Model(inputs=[channel_Wide,channel_Deep], outputs=[output])\n",
    "\n",
    "    # Compile\n",
    "    model_60.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])), \n",
    "        loss=GEH_loss, metrics=['MeanAbsoluteError','RootMeanSquaredError','MeanAbsolutePercentageError'])\n",
    "                    \n",
    "    return model_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner60 = RandomSearch(\n",
    "    build_model_60,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=30,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory=os.path.normpath('C:/Runs'),\n",
    "    project_name=\"7m-CNN-LSTM-60\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner60.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner60.search((Wt60,Xt60), yt60, epochs=200,\n",
    "            validation_split = val_percent,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner60.results_summary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp60 = tuner60.get_best_hyperparameters()[0]\n",
    "model60 = tuner60.hypermodel.build(best_hp60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/7m-ConvLSTM-60\"\n",
    "joblib.dump(best_hp60, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60.fit((Wt60,Xt60), yt60, \n",
    "            epochs = 100, \n",
    "            verbose = 1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60.save(\"my_dir/7m-CNN-LSTM-60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics60 = pd.DataFrame(model60.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics60[['loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60 = keras.models.load_model(\"my_dir/7m-CNN-LSTM-60\", custom_objects={'GEH_loss': GEH_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60.predict((Wv60,Xv60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions60 = model60.predict((Wv60,Xv60))\n",
    "\n",
    "# yv60 = scaler.inverse_transform(Test.drop('Timestamp', axis = 1))\n",
    "# yv60 = yv60[26:,3]\n",
    "\n",
    "# df = Test.drop(Test.head(26).index)\n",
    "# df.reset_index(inplace = True)\n",
    "# df = df.drop(columns=['index','Timestamp'])\n",
    "# df['f4'] = pd.Series(test_predictions60.flatten())\n",
    "\n",
    "# test_predictions60 = scaler.inverse_transform(df)\n",
    "\n",
    "# test_predictions60 = test_predictions60[:,3]\n",
    "\n",
    "# MSE60 = mean_squared_error(yv60,test_predictions60)\n",
    "\n",
    "# MAE60 = mean_absolute_error(yv60,test_predictions60)\n",
    "\n",
    "# RMSE60 = np.sqrt(MSE60)\n",
    "\n",
    "# MAPE60 = mean_absolute_percentage_error(yv60,test_predictions60)*100\n",
    "\n",
    "# eval60 = [MSE60, MAE60, RMSE60, MAPE60]\n",
    "# print(eval60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RA10 = ra.RA_10(np.asarray(yv60), np.asarray(test_predictions60))\n",
    "# RA15 = ra.RA_15(np.asarray(yv60), np.asarray(test_predictions60))\n",
    "# RA20 = ra.RA_20(np.asarray(yv60), np.asarray(test_predictions60))\n",
    "\n",
    "# res60 = [RA10, RA15, RA20]\n",
    "# print(res60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['MSE','MAE','RMSE','MAPE','Horizon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval5.append(5)\n",
    "eval15.append(15)\n",
    "eval30.append(30)\n",
    "eval60.append(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([eval5,eval15,eval30,eval60]),columns=l)\n",
    "df.set_index('Horizon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ['10%','15%','20%','Horizon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res5.append(5)\n",
    "res15.append(15)\n",
    "res30.append(30)\n",
    "res60.append(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.array([res5,res15,res30,res60]),columns=m)\n",
    "df1.set_index('Horizon')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
