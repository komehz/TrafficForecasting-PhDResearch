{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras_tuner import RandomSearch\n",
    "from GEH_LOSS import rGEH_loss\n",
    "import geh as ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import relative_accuracy as ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all Inputs and Output Data\n",
    "\n",
    "# 5 mins (1 step ahead)\n",
    "Deep_train_5   = np.load(\"Deep_train_5_multi.npz\")['x'] \n",
    "Output_train_5 = np.load(\"Deep_train_5_multi.npz\")['y']\n",
    "\n",
    "Deep_test_5   = np.load(\"Deep_test_5_multi.npz\")['x'] \n",
    "Output_test_5 = np.load(\"Deep_test_5_multi.npz\")['y'] \n",
    "\n",
    "# 15 mins (3 steps ahead)\n",
    "Deep_train_15   = np.load(\"Deep_train_15_multi.npz\")['x'] \n",
    "Output_train_15 = np.load(\"Deep_train_15_multi.npz\")['y']\n",
    "\n",
    "Deep_test_15   = np.load(\"Deep_test_15_multi.npz\")['x'] \n",
    "Output_test_15 = np.load(\"Deep_test_15_multi.npz\")['y']\n",
    "\n",
    "# 30 mins (6 steps ahead)\n",
    "Deep_train_30   = np.load(\"Deep_train_30_multi.npz\")['x'] \n",
    "Output_train_30 = np.load(\"Deep_train_30_multi.npz\")['y']\n",
    "\n",
    "Deep_test_30   = np.load(\"Deep_test_30_multi.npz\")['x'] \n",
    "Output_test_30 = np.load(\"Deep_test_30_multi.npz\")['y']\n",
    "\n",
    "# 60 mins (12 steps ahead)\n",
    "Deep_train_60   = np.load(\"Deep_train_60_multi.npz\")['x'] \n",
    "Output_train_60 = np.load(\"Deep_train_60_multi.npz\")['y']\n",
    "\n",
    "Deep_test_60   = np.load(\"Deep_test_60_multi.npz\")['x'] \n",
    "Output_test_60 = np.load(\"Deep_test_60_multi.npz\")['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide Data\n",
    "Xtrain = joblib.load(\"002weeks_train_Multi.save\") \n",
    "Xtest = joblib.load(\"002weeks_test_Multi.save\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 mins\n",
    "# Delete first 15 samples\n",
    "Wide_train_5 = np.delete(Xtrain, np.s_[0:15], 0)\n",
    "Wide_test_5 = np.delete(Xtest, np.s_[0:15], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 mins\n",
    "# Delete first 17 samples\n",
    "Wide_train_15 = np.delete(Xtrain, np.s_[0:17], 0)\n",
    "Wide_test_15 = np.delete(Xtest, np.s_[0:17], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 mins\n",
    "# Delete first 20 samples\n",
    "Wide_train_30 = np.delete(Xtrain, np.s_[0:20], 0)\n",
    "Wide_test_30 = np.delete(Xtest, np.s_[0:20], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60 mins\n",
    "# Delete first 27 samples\n",
    "Wide_train_60 = np.delete(Xtrain, np.s_[0:26], 0)\n",
    "Wide_test_60 = np.delete(Xtest, np.s_[0:26], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"scaler.save\"\n",
    "scaler = joblib.load(scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test      = pd.read_csv('01test_scaled.csv', index_col=None, parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from matplotlib import ticker\n",
    "formatter = ticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True) \n",
    "formatter.set_powerlimits((-1,1)) \n",
    "plt.rc('font', size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to split the input sequences into subsequences that can be processed by the CNN model. Here, each spatio-temporal sample can be split into three sub-samples, each with five time steps. The CNN can interpret each subsequence of five time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "n_features = 7                # No of loop detectors\n",
    "n_seq = 3                     # Subsequences\n",
    "n_steps = 5                   # time-step per subsequence\n",
    "# val_percent = 0.07567       # 2 weeks\n",
    "val_percent = 0.2        \n",
    "batch_size = 32\n",
    "\n",
    "n_seq_ = 3                     # Subsequences\n",
    "n_steps_ = 3                   # time-step per subsequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xt5 = Deep_train_5.reshape((Deep_train_5.shape[0], n_seq, n_steps, n_features))\n",
    "yt5 = scaler.inverse_transform(Output_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xv5 = Deep_test_5.reshape((Deep_test_5.shape[0], n_seq, n_steps, n_features))\n",
    "yv5 = scaler.inverse_transform(Output_test_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xt15 = Deep_train_15.reshape((Deep_train_15.shape[0], n_seq, n_steps, n_features))\n",
    "yt15 = scaler.inverse_transform(Output_train_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xv15 = Deep_test_15.reshape((Deep_test_15.shape[0], n_seq, n_steps, n_features))\n",
    "yv15 = scaler.inverse_transform(Output_test_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xt30 = Deep_train_30.reshape((Deep_train_30.shape[0], n_seq, n_steps, n_features))\n",
    "yt30 = scaler.inverse_transform(Output_train_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xv30 = Deep_test_30.reshape((Deep_test_30.shape[0], n_seq, n_steps, n_features))\n",
    "yv30 = scaler.inverse_transform(Output_test_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 60 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xt60 = Deep_train_60.reshape((Deep_train_60.shape[0], n_seq, n_steps, n_features))\n",
    "yt60 = scaler.inverse_transform(Output_train_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n",
    "Xv60 = Deep_test_60.reshape((Deep_test_60.shape[0], n_seq, n_steps, n_features))\n",
    "yv60 = scaler.inverse_transform(Output_test_60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Data for Conv2D layer\n",
    "\n",
    "For Conv2D, there is a need to add one more dimension to show we're dealing with 1 channel (since technically the images are in black and white, only showing values from 0-max flow on a single channel).\n",
    "\n",
    "Conv1D - strides in 1 dimension\n",
    "Conv2D - strides in 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define no_of_channels\n",
    "n_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "Xt5  =  Xt5.reshape(Xt5.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xt15 =  Xt15.reshape(Xt15.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xt30 =  Xt30.reshape(Xt30.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xt60 =  Xt60.reshape(Xt60.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "\n",
    "# Validation data\n",
    "Xv5  =  Xv5.reshape(Xv5.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xv15 =  Xv15.reshape(Xv15.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xv30 =  Xv30.reshape(Xv30.shape[0], n_seq, n_steps, n_features, n_channels)\n",
    "Xv60 =  Xv60.reshape(Xv60.shape[0], n_seq, n_steps, n_features, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wt5  = Wide_train_5\n",
    "Wt15 = Wide_train_15\n",
    "Wt30 = Wide_train_30\n",
    "Wt60 = Wide_train_60\n",
    "\n",
    "Wv5  = Wide_test_5\n",
    "Wv15 = Wide_test_15\n",
    "Wv30 = Wide_test_30\n",
    "Wv60 = Wide_test_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "Wt5  =  Wt5.reshape(Wt5.shape[0], n_seq_, n_steps_, n_features)\n",
    "Wt15 =  Wt15.reshape(Wt15.shape[0], n_seq_, n_steps_, n_features)\n",
    "Wt30 =  Wt30.reshape(Wt30.shape[0], n_seq_, n_steps_, n_features)\n",
    "Wt60 =  Wt60.reshape(Wt60.shape[0], n_seq_, n_steps_, n_features)\n",
    "\n",
    "# Validation data\n",
    "Wv5  =  Wv5.reshape(Wv5.shape[0], n_seq_, n_steps_, n_features)\n",
    "Wv15 =  Wv15.reshape(Wv15.shape[0], n_seq_, n_steps_, n_features)\n",
    "Wv30 =  Wv30.reshape(Wv30.shape[0], n_seq_, n_steps_, n_features)\n",
    "Wv60 =  Wv60.reshape(Wv60.shape[0], n_seq_, n_steps_, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wt5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "# Early Stopping\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wide_train_5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 mins ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_5(hp):\n",
    "    # Inputs\n",
    "    channel_Wide = keras.layers.Input(shape=Wt5.shape[1:], name=\"WideInput\")\n",
    "    channel_Deep = keras.layers.Input(shape=Xt5.shape[1:], name=\"DeepInput\")\n",
    "    units = hp.Int(\"units\", min_value=1, max_value=100, step=1)\n",
    "    \n",
    "    \n",
    "    # Wide Model\n",
    "    flatten_5w = keras.layers.TimeDistributed(keras.layers.Flatten())(channel_Wide)\n",
    "    Bi_5 = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(units=units, activation='relu', return_sequences=True),\n",
    "        merge_mode=\"ave\"\n",
    "    )(flatten_5w)\n",
    "    \n",
    "    \n",
    "    # Deep Model\n",
    "    # CNN \n",
    "    CNN_5 = keras.layers.TimeDistributed(\n",
    "        keras.layers.Conv2D(filters=hp.Int(\"filters\", min_value=32, max_value=512, step=32),\n",
    "                            kernel_size=hp.Choice(\"kernel_size\", [2, 3]), activation='relu'))(channel_Deep)\n",
    "    flatten_5 = keras.layers.TimeDistributed(keras.layers.Flatten())(CNN_5)\n",
    "    # LSTM          \n",
    "    LSTM_5 = keras.layers.LSTM(units=units, activation='relu', return_sequences=True)(flatten_5)\n",
    "\n",
    "    \n",
    "    # Concatenation \n",
    "    concat = keras.layers.concatenate([Bi_5, LSTM_5], axis=1)\n",
    "\n",
    "    # Attention\n",
    "    Att_5 = SeqSelfAttention(attention_activation='sigmoid')(concat)\n",
    "    merge = keras.layers.Flatten()(Att_5)\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    output = keras.layers.Dense(n_features, name= \"output\")(merge)\n",
    "    \n",
    "    # Model\n",
    "    model_5 = keras.Model(inputs=[channel_Wide,channel_Deep], outputs=[output])\n",
    "\n",
    "    # Compile\n",
    "    model_5.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])), \n",
    "        loss=rGEH_loss, metrics=['MeanAbsoluteError','RootMeanSquaredError','MeanAbsolutePercentageError'])\n",
    "                                   \n",
    "    return model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner5 = RandomSearch(\n",
    "    build_model_5,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=60,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=False,\n",
    "    directory=os.path.normpath('C:/Runs'),\n",
    "    project_name=\"8d-5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner5.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner5.search((Wt5,Xt5), yt5, epochs=200,\n",
    "           validation_split = val_percent,\n",
    "           callbacks = [early_stop],\n",
    "           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner5.results_summary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/8d-5\"\n",
    "best_hp5 = joblib.load(scaler_filename) \n",
    "\n",
    "model5 = tuner5.hypermodel.build(best_hp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp5 = tuner5.get_best_hyperparameters()[0]\n",
    "model5 = tuner5.hypermodel.build(best_hp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/8d-5\"\n",
    "joblib.dump(best_hp5, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = tuner5.hypermodel.build(joblib.load(\"Best_HP/8d-5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.fit((Wt5,Xt5), yt5, validation_split = val_percent,\n",
    "           epochs = 100, \n",
    "           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics5 = pd.DataFrame(model5.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "# Add set of axes to figure\n",
    "axes = fig.add_axes([0, 0, 1, 1]) \n",
    "axes2 = fig.add_axes([0.35, 0.45, 0.5, 0.5]) # Smaller figure\n",
    "axes.plot(metrics5[['loss','val_loss']], label=['train_loss','val_loss'], lw=4)\n",
    "\n",
    "axes2.plot(metrics5[['loss','val_loss']], lw=4)\n",
    "axes2.set_xlim(0,20)\n",
    "axes2.set_ylim(3.55,5)\n",
    "#plt.xticks(visible=False)\n",
    "plt.yticks(visible=False)\n",
    "\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')\n",
    "axes.grid()\n",
    "axes2.grid()\n",
    "\n",
    "mark_inset(axes, axes2, loc1=2, loc2=4, fc=\"0.9\", ec=\"0.1\", ls='--')\n",
    "axes.yaxis.set_major_formatter(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.save(\"my_dir/8d-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = keras.models.load_model(\"my_dir/8d-5\", custom_objects={'rGEH_loss': rGEH_loss}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.predict((Wv5,Xv5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions5 = model5.predict((Wv5,Xv5))\n",
    "\n",
    "\n",
    "MSE5 = mean_squared_error(yv5,test_predictions5)\n",
    "\n",
    "MAE5 = mean_absolute_error(yv5,test_predictions5)\n",
    "\n",
    "RMSE5 = np.sqrt(MSE5)\n",
    "\n",
    "MAPE5 = mean_absolute_percentage_error(yv5,test_predictions5)*100\n",
    "\n",
    "eval5 = [MSE5, MAE5, RMSE5, MAPE5]\n",
    "print(eval5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEH, count = ga.GEH(np.asarray(yv5), np.asarray(test_predictions5))\n",
    "GEH5 = count/GEH.size\n",
    "print(GEH5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 mins ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_15(hp):\n",
    "    # Inputs\n",
    "    channel_Wide = keras.layers.Input(shape=Wt15.shape[1:], name=\"WideInput\")\n",
    "    channel_Deep = keras.layers.Input(shape=Xt15.shape[1:], name=\"DeepInput\")\n",
    "    units = hp.Int(\"units\", min_value=1, max_value=100, step=1)\n",
    "    \n",
    "    \n",
    "    # Wide Model\n",
    "    flatten_15w = keras.layers.TimeDistributed(keras.layers.Flatten())(channel_Wide)\n",
    "    Bi_15 = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(units=units, activation='relu', return_sequences=True),\n",
    "        merge_mode=\"ave\"\n",
    "    )(flatten_15w)\n",
    "    \n",
    "    \n",
    "    # Deep Model\n",
    "    # CNN \n",
    "    CNN_15 = keras.layers.TimeDistributed(\n",
    "        keras.layers.Conv2D(filters=hp.Int(\"filters\", min_value=32, max_value=512, step=32),\n",
    "                            kernel_size=hp.Choice(\"kernel_size\", [2, 3]), activation='relu'))(channel_Deep)\n",
    "    flatten_15 = keras.layers.TimeDistributed(keras.layers.Flatten())(CNN_15)\n",
    "    # LSTM          \n",
    "    LSTM_15 = keras.layers.LSTM(units=units, activation='relu', return_sequences=True)(flatten_15)\n",
    "\n",
    "    \n",
    "    # Concatenation \n",
    "    concat = keras.layers.concatenate([Bi_15, LSTM_15], axis=1)\n",
    "\n",
    "    # Attention\n",
    "    Att_15 = SeqSelfAttention(attention_activation='sigmoid')(concat)\n",
    "    merge = keras.layers.Flatten()(Att_15)\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    output = keras.layers.Dense(n_features, name= \"output\")(merge)\n",
    "    \n",
    "    # Model\n",
    "    model_15 = keras.Model(inputs=[channel_Wide,channel_Deep], outputs=[output])\n",
    "\n",
    "    # Compile\n",
    "    model_15.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])), \n",
    "        loss=rGEH_loss, metrics=['MeanAbsoluteError','RootMeanSquaredError','MeanAbsolutePercentageError'])\n",
    "                    \n",
    "    return model_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner15 = RandomSearch(\n",
    "    build_model_15,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=60,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=False,\n",
    "    directory=os.path.normpath('C:/Runs'),\n",
    "    project_name=\"8d-15\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner15.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuner15.search((Wt15,Xt15), yt15, epochs=200,\n",
    "            validation_split = val_percent,\n",
    "            callbacks=[early_stop], \n",
    "            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner15.results_summary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/8d-15\"\n",
    "best_hp15 = joblib.load(scaler_filename) \n",
    "\n",
    "model15 = tuner15.hypermodel.build(best_hp15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp15 = tuner15.get_best_hyperparameters()[0]\n",
    "model15 = tuner15.hypermodel.build(best_hp15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/8d-15\"\n",
    "joblib.dump(best_hp15, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15 = tuner15.hypermodel.build(joblib.load(\"Best_HP/8d-15\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15.fit((Wt15,Xt15), yt15, validation_split = val_percent,\n",
    "            epochs = 100, \n",
    "            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics15 = pd.DataFrame(model15.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "# Add set of axes to figure\n",
    "axes = fig.add_axes([0, 0, 1, 1]) \n",
    "#axes2 = fig.add_axes([0.35, 0.45, 0.5, 0.5]) # Smaller figure\n",
    "axes.plot(metrics15[['loss','val_loss']], label=['train_loss','val_loss'], lw=3)\n",
    "\n",
    "#axes2.plot(metrics15[['loss','val_loss']], lw=3)\n",
    "#axes2.set_xlim(0,20)\n",
    "#axes2.set_ylim(4,6)\n",
    "#plt.xticks(visible=False)\n",
    "#plt.yticks(visible=False)\n",
    "\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')\n",
    "axes.grid()\n",
    "#axes2.grid()\n",
    "\n",
    "#mark_inset(axes, axes2, loc1=2, loc2=4, fc=\"0.9\", ec=\"0.1\", ls='--')\n",
    "axes.yaxis.set_major_formatter(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15.save(\"my_dir/8d-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15 = keras.models.load_model(\"my_dir/8d-15\", custom_objects={'rGEH_loss': rGEH_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15.predict((Wv15,Xv15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions15 = model15.predict((Wv15,Xv15))\n",
    "\n",
    "\n",
    "MSE15 = mean_squared_error(yv15,test_predictions15)\n",
    "\n",
    "MAE15 = mean_absolute_error(yv15,test_predictions15)\n",
    "\n",
    "RMSE15 = np.sqrt(MSE15)\n",
    "\n",
    "MAPE15 = mean_absolute_percentage_error(yv15,test_predictions15)*100\n",
    "\n",
    "eval15 = [MSE15, MAE15, RMSE15, MAPE15]\n",
    "print(eval15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEH, count = ga.GEH(np.asarray(yv15), np.asarray(test_predictions15))\n",
    "GEH15 = count/GEH.size\n",
    "print(GEH15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 mins ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_30(hp):\n",
    "    # Inputs\n",
    "    channel_Wide = keras.layers.Input(shape=Wt30.shape[1:], name=\"WideInput\")\n",
    "    channel_Deep = keras.layers.Input(shape=Xt30.shape[1:], name=\"DeepInput\")\n",
    "    units = hp.Int(\"units\", min_value=1, max_value=100, step=1)\n",
    "    \n",
    "    \n",
    "    # Wide Model\n",
    "    flatten_30w = keras.layers.TimeDistributed(keras.layers.Flatten())(channel_Wide)\n",
    "    Bi_30 = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(units=units, activation='relu', return_sequences=True),\n",
    "        merge_mode=\"ave\"\n",
    "    )(flatten_30w)\n",
    "    \n",
    "    \n",
    "    # Deep Model\n",
    "    # CNN \n",
    "    CNN_30 = keras.layers.TimeDistributed(\n",
    "        keras.layers.Conv2D(filters=hp.Int(\"filters\", min_value=32, max_value=512, step=32),\n",
    "                            kernel_size=hp.Choice(\"kernel_size\", [2, 3]), activation='relu'))(channel_Deep)\n",
    "    flatten_30 = keras.layers.TimeDistributed(keras.layers.Flatten())(CNN_30)\n",
    "    # LSTM          \n",
    "    LSTM_30 = keras.layers.LSTM(units=units, activation='relu', return_sequences=True)(flatten_30)\n",
    "\n",
    "    \n",
    "    # Concatenation \n",
    "    concat = keras.layers.concatenate([Bi_30, LSTM_30], axis=1)\n",
    "\n",
    "    # Attention\n",
    "    Att_30 = SeqSelfAttention(attention_activation='sigmoid')(concat)\n",
    "    merge = keras.layers.Flatten()(Att_30)\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    output = keras.layers.Dense(n_features, name= \"output\")(merge)\n",
    "    \n",
    "    # Model\n",
    "    model_30 = keras.Model(inputs=[channel_Wide,channel_Deep], outputs=[output])\n",
    "\n",
    "    # Compile\n",
    "    model_30.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])), \n",
    "        loss=rGEH_loss, metrics=['MeanAbsoluteError','RootMeanSquaredError','MeanAbsolutePercentageError'])\n",
    "                    \n",
    "    return model_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner30 = RandomSearch(\n",
    "    build_model_30,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=60,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=False,\n",
    "    directory=os.path.normpath('C:/Runs'),\n",
    "    project_name=\"8d-30\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner30.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner30.search((Wt30,Xt30), yt30, epochs=200,\n",
    "            validation_split = val_percent,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner30.results_summary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/8d-30\"\n",
    "best_hp30 = joblib.load(scaler_filename) \n",
    "\n",
    "model30 = tuner30.hypermodel.build(best_hp30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp30 = tuner30.get_best_hyperparameters()[0]\n",
    "model30 = tuner30.hypermodel.build(best_hp30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"8d-30\"\n",
    "joblib.dump(best_hp30, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30.fit((Wt30,Xt30), yt30, validation_split = val_percent,\n",
    "            epochs = 100, \n",
    "            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics30 = pd.DataFrame(model30.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "# Add set of axes to figure\n",
    "axes = fig.add_axes([0, 0, 1, 1]) \n",
    "#axes2 = fig.add_axes([0.35, 0.45, 0.5, 0.5]) # Smaller figure\n",
    "axes.plot(metrics30[['loss','val_loss']], label=['train_loss','val_loss'], lw=4)\n",
    "\n",
    "#axes2.plot(metrics30[['loss','val_loss']], lw=4)\n",
    "#axes2.set_xlim(0,20)\n",
    "#axes2.set_ylim(4,6)\n",
    "#plt.xticks(visible=False)\n",
    "# plt.yticks(visible=False)\n",
    "\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')\n",
    "axes.grid()\n",
    "#axes2.grid()\n",
    "\n",
    "#mark_inset(axes, axes2, loc1=2, loc2=4, fc=\"0.9\", ec=\"0.1\", ls='--')\n",
    "axes.yaxis.set_major_formatter(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30.save(\"my_dir/8d-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30 = keras.models.load_model(\"my_dir/8d-30\", custom_objects={'rGEH_loss': rGEH_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model30.predict((Wv30,Xv30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions30 = model30.predict((Wv30,Xv30))\n",
    "\n",
    "\n",
    "MSE30 = mean_squared_error(yv30,test_predictions30)\n",
    "\n",
    "MAE30 = mean_absolute_error(yv30,test_predictions30)\n",
    "\n",
    "RMSE30 = np.sqrt(MSE30)\n",
    "\n",
    "MAPE30 = mean_absolute_percentage_error(yv30,test_predictions30)*100\n",
    "\n",
    "eval30 = [MSE30, MAE30, RMSE30, MAPE30]\n",
    "print(eval30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEH, count = ga.GEH(np.asarray(yv30), np.asarray(test_predictions30))\n",
    "GEH30 = count/GEH.size\n",
    "print(GEH30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 60 mins ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_60(hp):\n",
    "    # Inputs\n",
    "    channel_Wide = keras.layers.Input(shape=Wt60.shape[1:], name=\"WideInput\")\n",
    "    channel_Deep = keras.layers.Input(shape=Xt60.shape[1:], name=\"DeepInput\")\n",
    "    units = hp.Int(\"units\", min_value=1, max_value=100, step=1)\n",
    "    \n",
    "    \n",
    "    # Wide Model\n",
    "    flatten_60w = keras.layers.TimeDistributed(keras.layers.Flatten())(channel_Wide)\n",
    "    Bi_60 = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(units=units, activation='relu', return_sequences=True),\n",
    "        merge_mode=\"ave\"\n",
    "    )(flatten_60w)\n",
    "    \n",
    "    \n",
    "    # Deep Model\n",
    "    # CNN \n",
    "    CNN_60 = keras.layers.TimeDistributed(\n",
    "        keras.layers.Conv2D(filters=hp.Int(\"filters\", min_value=32, max_value=512, step=32),\n",
    "                            kernel_size=hp.Choice(\"kernel_size\", [2, 3]), activation='relu'))(channel_Deep)\n",
    "    flatten_60 = keras.layers.TimeDistributed(keras.layers.Flatten())(CNN_60)\n",
    "    # LSTM          \n",
    "    LSTM_60 = keras.layers.LSTM(units=units, activation='relu', return_sequences=True)(flatten_60)\n",
    "\n",
    "    \n",
    "    # Concatenation \n",
    "    concat = keras.layers.concatenate([Bi_60, LSTM_60], axis=1)\n",
    "\n",
    "    # Attention\n",
    "    Att_60 = SeqSelfAttention(attention_activation='sigmoid')(concat)\n",
    "    merge = keras.layers.Flatten()(Att_60)\n",
    "\n",
    "    \n",
    "    # Output\n",
    "    output = keras.layers.Dense(n_features, name= \"output\")(merge)\n",
    "    \n",
    "    # Model\n",
    "    model_60 = keras.Model(inputs=[channel_Wide,channel_Deep], outputs=[output])\n",
    "\n",
    "    # Compile\n",
    "    model_60.compile(optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])), \n",
    "        loss=rGEH_loss, metrics=['MeanAbsoluteError','RootMeanSquaredError','MeanAbsolutePercentageError'])\n",
    "                    \n",
    "    return model_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner60 = RandomSearch(\n",
    "    build_model_60,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=60,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=False,\n",
    "    directory=os.path.normpath('C:/Runs'),\n",
    "    project_name=\"8d-60\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner60.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner60.search((Wt60,Xt60), yt60, epochs=200,\n",
    "            validation_split = val_percent,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner60.results_summary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/8d-60\"\n",
    "best_hp60 = joblib.load(scaler_filename) \n",
    "\n",
    "model60 = tuner60.hypermodel.build(best_hp60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp60 = tuner60.get_best_hyperparameters()[0]\n",
    "model60 = tuner60.hypermodel.build(best_hp60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"Best_HP/8d-60\"\n",
    "joblib.dump(best_hp60, scaler_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60 = tuner60.hypermodel.build(joblib.load(\"Best_HP/8d-60\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60.fit((Wt60,Xt60), yt60, validation_split = val_percent,\n",
    "            epochs = 100, \n",
    "            verbose = 1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics60 = pd.DataFrame(model60.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "\n",
    "# Add set of axes to figure\n",
    "axes = fig.add_axes([0, 0, 1, 1]) \n",
    "#axes2 = fig.add_axes([0.35, 0.45, 0.5, 0.5]) # Smaller figure\n",
    "axes.plot(metrics60[['loss','val_loss']], label=['train_loss','val_loss'], lw=3)\n",
    "\n",
    "#axes2.plot(metrics60[['loss','val_loss']], lw=3)\n",
    "#axes2.set_xlim(0,25)\n",
    "#axes2.set_ylim(4.5,7)\n",
    "#plt.xticks(visible=False)\n",
    "#plt.yticks(visible=False)\n",
    "\n",
    "axes.set_xlabel('Epoch')\n",
    "axes.set_ylabel('Loss')\n",
    "axes.grid()\n",
    "#axes2.grid()\n",
    "\n",
    "#mark_inset(axes, axes2, loc1=2, loc2=4, fc=\"0.9\", ec=\"0.1\", ls='--')\n",
    "axes.yaxis.set_major_formatter(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60.save(\"my_dir/8d-60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60 = keras.models.load_model(\"my_dir/8d-60\", custom_objects={'rGEH_loss': rGEH_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model60.predict((Wv60,Xv60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions60 = model60.predict((Wv60,Xv60))\n",
    "\n",
    "\n",
    "MSE60 = mean_squared_error(yv60,test_predictions60)\n",
    "\n",
    "MAE60 = mean_absolute_error(yv60,test_predictions60)\n",
    "\n",
    "RMSE60 = np.sqrt(MSE60)\n",
    "\n",
    "MAPE60 = mean_absolute_percentage_error(yv60,test_predictions60)*100\n",
    "\n",
    "eval60 = [MSE60, MAE60, RMSE60, MAPE60]\n",
    "print(eval60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEH, count = ga.GEH(np.asarray(yv60), np.asarray(test_predictions60))\n",
    "GEH60 = count/GEH.size\n",
    "print(GEH60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['MSE','MAE','RMSE','MAPE','Horizon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval5.append(5)\n",
    "eval15.append(15)\n",
    "eval30.append(30)\n",
    "eval60.append(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([eval5,eval15,eval30,eval60]),columns=l)\n",
    "df.set_index('Horizon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [' 5 ',' 15 ',' 30 ',' 60 ']\n",
    "df2 = pd.DataFrame(np.array([[GEH5, GEH15, GEH30, GEH60]]), columns=n)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics5.to_csv('Loss/8d-5.csv') \n",
    "metrics15.to_csv('Loss/8d-15.csv') \n",
    "metrics30.to_csv('Loss/8d-30.csv') \n",
    "metrics60.to_csv('Loss/8d-60.csv') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
